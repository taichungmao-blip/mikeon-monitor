import requests
from bs4 import BeautifulSoup
import os
import json
import re
from datetime import datetime

# ================= è¨­å®šå€ =================
# å…¥å£ç¶²å€ (ç¬¬ä¸€é )
BASE_URL = "https://stocks.ddns.net/Forum/128/mikeon88%E6%8C%81%E8%82%A1%E5%A4%A7%E5%85%AC%E9%96%8B.aspx"
STATUS_FILE = "status.json"
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}
# ==============================================

def send_discord_notify(message_content, post_time, url):
    if not DISCORD_WEBHOOK_URL:
        print("âŒ æœªè¨­å®š Discord Webhook")
        return

    preview = message_content[:300] + "..." if len(message_content) > 300 else message_content
    
    data = {
        "username": "Mikeon88 è¿½è¹¤å™¨",
        "embeds": [{
            "title": "ğŸš¨ Mikeon88 æœ‰æ–°ç™¼è¨€ï¼",
            "description": preview,
            "url": url,
            "color": 15158332, 
            "fields": [
                {"name": "ç™¼è¨€æ™‚é–“", "value": post_time, "inline": True},
                {"name": "ä¾†æºé€£çµ", "value": f"[é»æ“Šå‰å¾€]({url})", "inline": True}
            ],
            "footer": {
                "text": "V5 JavaScript ç¹é“ç‰ˆ"
            }
        }]
    }
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=data)
        print("âœ… Discord é€šçŸ¥å·²ç™¼é€")
    except Exception as e:
        print(f"âŒ ç™¼é€å¤±æ•—: {e}")

def load_status():
    if os.path.exists(STATUS_FILE):
        with open(STATUS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"last_fingerprint": ""}

def save_status(fingerprint):
    with open(STATUS_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_fingerprint": fingerprint}, f, ensure_ascii=False, indent=4)

def get_max_page_number(soup):
    """
    å¾åˆ†é åˆ—ä¸­è§£æå‡ºæœ€å¤§çš„é ç¢¼æ•¸å­—
    """
    print("ğŸ” åˆ†æåˆ†é çµæ§‹...")
    max_page = 1
    
    # ç­–ç•¥1ï¼šç›´æ¥çœ‹æŒ‰éˆ•çš„æ–‡å­— (ä¾‹å¦‚ "23")
    links = soup.find_all("a", href=True)
    for link in links:
        txt = link.get_text(strip=True)
        if txt.isdigit():
            val = int(txt)
            if val > max_page:
                max_page = val
    
    # ç­–ç•¥2ï¼šå¦‚æœæœ€å¾Œä¸€é æ˜¯ "..." æˆ– "Last"ï¼Œå˜—è©¦å¾ href çš„ JS åƒæ•¸ä¸­æŒ–æ•¸å­—
    # ASP.NET å¸¸è¦‹æ ¼å¼: javascript:__doPostBack('...','Page$23')
    for link in links:
        href = link['href']
        if "Page$" in href:
            match = re.search(r"Page\$(\d+)", href)
            if match:
                val = int(match.group(1))
                if val > max_page:
                    max_page = val
                    
    print(f"ğŸ“Š åµæ¸¬åˆ°æœ€å¤§é æ•¸ç‚º: {max_page}")
    return max_page

def main():
    print(f"ğŸš€ V5 å•Ÿå‹•æª¢æŸ¥: {datetime.now()}")
    
    status = load_status()
    last_fingerprint = status["last_fingerprint"]
    
    # æ­¥é©Ÿ 1: é€²å…¥ç¬¬ä¸€é 
    print(f"1ï¸âƒ£ è®€å–å…¥å£é é¢...")
    try:
        session = requests.Session()
        res = session.get(BASE_URL, headers=HEADERS, timeout=20)
        res.encoding = 'utf-8'
        
        if res.status_code != 200:
            print(f"âŒ å…¥å£ç¶²é è®€å–å¤±æ•—: {res.status_code}")
            return

        soup = BeautifulSoup(res.text, "html.parser")
        
        # æ­¥é©Ÿ 2: è¨ˆç®—æœ€å¤§é æ•¸ä¸¦æ‰‹å‹•çµ„ç¶²å€
        max_page = get_max_page_number(soup)
        
        # çµ„åˆç¶²å€ (ç¹é JavaScript)
        target_url = f"{BASE_URL}?page={max_page}"
        print(f"2ï¸âƒ£ é–å®šç›®æ¨™ç¶²å€: {target_url}")
        
        # å¦‚æœç›®æ¨™é ä¸æ˜¯ç¬¬ä¸€é ï¼Œå°±é€²è¡Œè·³è½‰
        if max_page > 1:
            print(f"ğŸš€ è·³è½‰è‡³ç¬¬ {max_page} é ...")
            res = session.get(target_url, headers=HEADERS, timeout=20)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, "html.parser")

        # æ­¥é©Ÿ 3: æœå°‹ Mikeon88 (V3 ç²¾æº–é‚è¼¯)
        author_links = soup.find_all("a", id=re.compile("lnkName"))
        found_posts = []
        print(f"ğŸ” æƒæç™¼è¨€ä¸­...")

        for author in author_links:
            author_name = author.get_text(strip=True)
            if "mikeon88" in author_name.lower():
                container = author
                post_content = "ç„¡å…§å®¹"
                post_time = "ç„¡æ™‚é–“"
                
                # å¾€ä¸Šæ‰¾å®¹å™¨
                for _ in range(5):
                    if container.parent:
                        container = container.parent
                        body_div = container.find("div", class_="post-body")
                        if body_div:
                            post_content = body_div.get_text("\n", strip=True)
                        time_span = container.find("span", class_="local-time")
                        if time_span:
                            post_time = time_span.text.strip()
                        if body_div: break
                    else: break
                
                if post_content != "ç„¡å…§å®¹":
                    found_posts.append({"time": post_time, "content": post_content})

        if not found_posts:
            print("ğŸ’¤ æœ¬é æ²’æœ‰ Mikeon88 çš„ç™¼è¨€")
            save_status(last_fingerprint)
            return

        # æ­¥é©Ÿ 4: é–å®šæœ€æ–°ç™¼è¨€
        latest = found_posts[-1]
        
        print(f"ğŸ” æœ€æ–°ç™¼è¨€æ™‚é–“: {latest['time']}")
        
        current_fingerprint = f"{latest['time']}_{latest['content'][:30]}"
        
        if current_fingerprint != last_fingerprint:
            print(f"ğŸ‰ ç™¼ç¾æ–°å…§å®¹ï¼ç™¼é€é€šçŸ¥...")
            send_discord_notify(latest['content'], latest['time'], target_url)
            save_status(current_fingerprint)
        else:
            print("ğŸ’¤ å…§å®¹èˆ‡ä¸Šæ¬¡ç›¸åŒï¼Œè·³éé€šçŸ¥")
            save_status(last_fingerprint)

    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
