import requests
from bs4 import BeautifulSoup
import os
import json
import re
from datetime import datetime
import time

# ================= è¨­å®šå€ =================
BASE_URL = "https://stocks.ddns.net/Forum/128/mikeon88%E6%8C%81%E8%82%A1%E5%A4%A7%E5%85%AC%E9%96%8B.aspx"
STATUS_FILE = "status.json"
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}
# ==============================================

def send_discord_notify(message_content, post_time, url):
    if not DISCORD_WEBHOOK_URL:
        print("âŒ æœªè¨­å®š Discord Webhook")
        return

    preview = message_content[:300] + "..." if len(message_content) > 300 else message_content
    
    data = {
        "username": "Mikeon88 è¿½è¹¤å™¨",
        "embeds": [{
            "title": "ğŸš¨ Mikeon88 æœ‰æ–°ç™¼è¨€ï¼",
            "description": preview,
            "url": url,
            "color": 15158332, 
            "fields": [
                {"name": "ç™¼è¨€æ™‚é–“", "value": post_time, "inline": True},
                {"name": "ä¾†æºé€£çµ", "value": f"[é»æ“Šå‰å¾€]({url})", "inline": True}
            ],
            "footer": {
                "text": "V10 ç„¡ç›¡æ”€ç™»ç‰ˆ"
            }
        }]
    }
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=data)
        print("âœ… Discord é€šçŸ¥å·²ç™¼é€")
    except Exception as e:
        print(f"âŒ ç™¼é€å¤±æ•—: {e}")

def load_status():
    if os.path.exists(STATUS_FILE):
        with open(STATUS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"last_fingerprint": ""}

def save_status(fingerprint):
    with open(STATUS_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_fingerprint": fingerprint}, f, ensure_ascii=False, indent=4)

def get_hidden_fields(soup):
    data = {}
    for item in ["__VIEWSTATE", "__VIEWSTATEGENERATOR", "__EVENTVALIDATION"]:
        element = soup.find("input", {"id": item})
        if element:
            data[item] = element.get("value")
    return data

def extract_do_postback_args(href):
    if not href: return None, None
    match = re.search(r"__doPostBack\(['\"]([^'\"]*)['\"],\s*['\"]([^'\"]*)['\"]\)", href)
    if match:
        return match.group(1), match.group(2)
    return None, None

def get_current_page_num(soup):
    """å˜—è©¦æ‰¾å‡ºç›®å‰é é¢æ˜¯ç¬¬å¹¾é """
    # æ–¹æ³•ï¼šé€šå¸¸ç•¶å‰é ç¢¼çš„æŒ‰éˆ•æ˜¯æ²’æœ‰ href çš„ï¼Œæˆ–è€…æœ‰ç‰¹å®š class
    # æˆ‘å€‘æª¢æŸ¥åˆ†é å€å¡Š
    try:
        # å°‹æ‰¾åˆ†é å€å¡Š (é€šå¸¸åœ¨ table æˆ– div è£¡)
        # é€™è£¡æˆ‘å€‘æ‰¾æ‰€æœ‰æ•¸å­—æŒ‰éˆ•ï¼Œçœ‹çœ‹å“ªå€‹æ²’æœ‰ href (ä»£è¡¨æ˜¯ç•¶å‰é )
        # æˆ–è€…è¢« span åŒ…ä½çš„æ•¸å­—
        pager_active = soup.find("span", style=re.compile(r"font-weight:bold|color:Red", re.I))
        if pager_active and pager_active.text.isdigit():
             return int(pager_active.text)
        
        # å‚™ç”¨æ–¹æ¡ˆï¼šæœ‰äº›ç¶²ç«™ç•¶å‰é åªæ˜¯ç´”æ–‡å­—ï¼Œä¸æ˜¯é€£çµ
        # æˆ‘å€‘å‡è¨­å¦‚æœæ‰¾ä¸åˆ°ç•¶å‰é ï¼Œå°±å›å‚³ 0ï¼Œè®“ç¨‹å¼ä¾é æœ€å¤§æ•¸å­—å»è·³
        return 0
    except:
        return 0

def chase_last_page(session):
    print("1ï¸âƒ£ é€²å…¥å…¥å£é é¢...")
    res = session.get(BASE_URL, headers=HEADERS, timeout=30)
    soup = BeautifulSoup(res.text, "html.parser")
    
    current_page = 1
    max_hops = 15 # å¢åŠ è·³è½‰æ¬¡æ•¸ä¸Šé™
    
    for hop in range(max_hops):
        # å˜—è©¦è­˜åˆ¥ç•¶å‰é 
        detected_page = get_current_page_num(soup)
        if detected_page > current_page:
            current_page = detected_page
        
        print(f"ğŸƒ ç¬¬ {hop + 1} æ¬¡è·³è½‰åˆ†æ (ç›®å‰ç´„åœ¨ Page {current_page})...")
        
        links = soup.find_all("a", href=re.compile(r"__doPostBack"))
        
        best_link = None
        best_arg_val = -1
        target_type = "None"
        
        # æƒææ‰€æœ‰æŒ‰éˆ•ï¼Œå°‹æ‰¾æœ€ä½³è·³è½‰ç›®æ¨™
        for link in links:
            target, arg = extract_do_postback_args(link['href'])
            txt = link.get_text(strip=True)
            
            # è§£æåƒæ•¸ (æ ¼å¼é€šå¸¸æ˜¯ Page$11 æˆ– Page$Last)
            if arg and arg.startswith("Page$"):
                val_str = arg.replace("Page$", "")
                
                # å„ªå…ˆç´š S: ç›´æ¥æ˜¯ Last
                if val_str == "Last" or "Last" in txt or "æœ«é " in txt:
                    best_link = link
                    target_type = "Last"
                    break # æ‰¾åˆ°æœ€å¾Œä¸€é ï¼Œç›´æ¥é–å®š
                
                # å„ªå…ˆç´š A: æ•¸å­—
                if val_str.isdigit():
                    page_num = int(val_str)
                    # åªæœ‰ç•¶é€™å€‹æ•¸å­—ã€Œå¤§æ–¼ã€æˆ‘å€‘ç›®å‰æ‰€åœ¨çš„é æ•¸æ™‚ï¼Œæ‰è€ƒæ…®
                    if page_num > current_page and page_num > best_arg_val:
                        best_arg_val = page_num
                        best_link = link
                        target_type = f"Page {page_num}"
            
            # å„ªå…ˆç´š B: åªæœ‰æ–‡å­—ç‰¹å¾µ (>> æˆ– ...)
            elif ">>" in txt or "..." in txt:
                # åªæœ‰ç•¶æˆ‘å€‘é‚„æ²’æ‰¾åˆ°æ˜ç¢ºçš„æ•¸å­—ç›®æ¨™æ™‚ï¼Œæ‰æŠŠé€™å€‹ç•¶å‚™æ¡ˆ
                if target_type == "None":
                    best_link = link
                    target_type = "Next Block"

        # æ±ºç­–åŸ·è¡Œ
        if best_link:
            print(f"ğŸ¯ é–å®šç›®æ¨™: [{target_type}]ï¼ŒåŸ·è¡Œè·³è½‰...")
            target, arg = extract_do_postback_args(best_link['href'])
            
            payload = get_hidden_fields(soup)
            payload["__EVENTTARGET"] = target
            payload["__EVENTARGUMENT"] = arg
            
            post_res = session.post(BASE_URL, data=payload, headers=HEADERS, timeout=30)
            if post_res.status_code == 200:
                soup = BeautifulSoup(post_res.text, "html.parser")
                # æ›´æ–°ç•¶å‰é ç¢¼ç´€éŒ„ (å¦‚æœæ˜¯è·³æ•¸å­—çš„è©±)
                if target_type.startswith("Page "):
                    current_page = int(target_type.split()[1])
                elif target_type == "Next Block":
                    current_page += 1 # é ä¼°å‰é€²äº†
                print("âœ… è·³è½‰æˆåŠŸï¼")
                time.sleep(1)
            else:
                print(f"âŒ è·³è½‰å¤±æ•—: {post_res.status_code}")
                break
        else:
            print("ğŸ ç„¡æ³•æ‰¾åˆ°æ›´å¾Œé¢çš„é é¢ï¼Œåˆ¤æ–·å·²é”ã€çµ‚é»ã€‘")
            break
            
    return soup

def extract_time(container):
    time_span = container.find("span", class_="local-time")
    if time_span: return time_span.text.strip()
    text = container.get_text()
    match = re.search(r'\d{4}/\d{1,2}/\d{1,2}\s+\d{1,2}:\d{1,2}:\d{1,2}', text)
    if match: return match.group(0)
    return "ç„¡æ™‚é–“è³‡è¨Š"

def main():
    print(f"ğŸš€ V10 å•Ÿå‹•æª¢æŸ¥: {datetime.now()}")
    status = load_status()
    last_fingerprint = status["last_fingerprint"]
    
    session = requests.Session()
    
    # 1. åŸ·è¡Œè¿½é 
    soup = chase_last_page(session)

    # 2. æœå°‹ Mikeon88
    author_links = soup.find_all("a", id=re.compile("lnkName"))
    found_posts = []
    print(f"ğŸ” æƒææœ€çµ‚é é¢ç™¼è¨€...")

    for author in author_links:
        author_name = author.get_text(strip=True)
        if "mikeon88" in author_name.lower():
            container = author
            post_content = "ç„¡å…§å®¹"
            post_time = "ç„¡æ™‚é–“"
            
            for _ in range(5):
                if container.parent:
                    container = container.parent
                    body_div = container.find("div", class_="post-body")
                    if body_div:
                        post_content = body_div.get_text("\n", strip=True)
                    t = extract_time(container)
                    if t != "ç„¡æ™‚é–“è³‡è¨Š": post_time = t
                    if body_div: break
                else: break
            
            if post_content != "ç„¡å…§å®¹":
                found_posts.append({"time": post_time, "content": post_content})

    if not found_posts:
        print("ğŸ’¤ æœ¬é æ²’æœ‰ Mikeon88 çš„ç™¼è¨€")
        save_status(last_fingerprint)
        return

    # 3. é–å®šæœ€æ–°ç™¼è¨€
    latest = found_posts[-1]
    print(f"ğŸ” æœ€æ–°ç™¼è¨€æ™‚é–“: {latest['time']}")
    print(f"ğŸ“ å…§å®¹é è¦½: {latest['content'][:30]}...")
    
    current_fingerprint = f"{latest['time']}_{latest['content'][:30]}"
    
    if current_fingerprint != last_fingerprint:
        print(f"ğŸ‰ ç™¼ç¾æ–°å…§å®¹ï¼ç™¼é€é€šçŸ¥...")
        send_discord_notify(latest['content'], latest['time'], BASE_URL)
        save_status(current_fingerprint)
    else:
        print("ğŸ’¤ å…§å®¹èˆ‡ä¸Šæ¬¡ç›¸åŒï¼Œè·³éé€šçŸ¥")
        save_status(last_fingerprint)

if __name__ == "__main__":
    main()
